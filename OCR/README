In fileutil.py and settings.sh the path of the data is set, say $DATA.

$DATA/scans

contains:
* [files of form:] scan_123.tif
* [files of form:] exclude_123.txt

Files of form exclude_123.txt contain lines of form 010003.bin.png,
corresponding to lines automatically extracted from the scan that need
to be ignored.

$DATA/trans

contains:
* [files of form:] trans_123.txt

$DATA/codecs

contains:
* ordinaries.txt
* all.txt
* names.txt

The file ordinaries.txt is maintained by hand. It contains ordinary
characters, which correspond to a single key on a normal keyboard.
The file all.txt may be overwritten automatically, as extracted
from the transcriptions. The file named.txt is a human readable
version, with names of glyphs.

$DATA/bin

contains reproducible files. Must be initialised to contain the directory structure.

$DATA/models

contains models resulting from training.

$DATA/backup

may be used for storing backups.

=========================================================================

Pipeline:

* prep.sh, after adjusting FIRST and LAST

* sellines.py, after adjusting page

* train.sh, after adjusting FIRST and LAST

* test.sh, after adjusting FIRST and LAST, and MODEL

* eval.sh, after adjusting FIRST and LAST

* edittrans.py, after adjusting page

Maintenance of characters in transcriptions:

* getcodec.py, and inspect $DATA/codecs/names.txt

* subtrans.py firstpage lastpage oldgrapheme newgrapheme

which requires manually inspecting files in $DATA/trans/ .
The changed files are of the form trans_1234_new.txt , which can
be compare to files of the form trans_1234.txt . When this is approved,
the change is made permanent by:

* subtrans.py firstpage lastpage oldgrapheme newgrapheme confirm

Postprocessing

* postproc.py, after adjusting page

which produces a file with transcriptions linked to rectangles.

=========================================================================

Notes:

There is a bug in ocropus-rtrain that intermittently causes it to crash when
training on large data sets. If training fails, decrease the size of the training
set.

